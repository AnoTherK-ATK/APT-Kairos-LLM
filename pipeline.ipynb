{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Config",
   "id": "d67c9eec2c9316be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "#\n",
    "#                   Artifacts path\n",
    "#\n",
    "########################################################\n",
    "\n",
    "# The directory of the raw logs\n",
    "raw_dir = \"/home/anotherk/kairos312/DARPA/CADETS_E3/e3/\"\n",
    "\n",
    "# The directory to save all artifacts\n",
    "artifact_dir = \"./artifact/\"\n",
    "\n",
    "# The directory to save the vectorized graphs\n",
    "graphs_dir = artifact_dir + \"graphs/\"\n",
    "\n",
    "# The directory to save the models\n",
    "models_dir = artifact_dir + \"models/\"\n",
    "\n",
    "# The directory to save the results after testing\n",
    "test_re = artifact_dir + \"test_re/\"\n",
    "\n",
    "# The directory to save all visualized results\n",
    "vis_re = artifact_dir + \"vis_re/\"\n",
    "\n",
    "\n",
    "\n",
    "########################################################\n",
    "#\n",
    "#               Database settings\n",
    "#\n",
    "########################################################\n",
    "\n",
    "# Database name\n",
    "database = 'tc_cadet_dataset_db'\n",
    "\n",
    "# Only config this setting when you have the problem mentioned\n",
    "# in the Troubleshooting section in settings/environment-settings.md.\n",
    "# Otherwise, set it as None\n",
    "host = '/var/run/postgresql/'\n",
    "# host = None\n",
    "\n",
    "# Database user\n",
    "user = 'postgres'\n",
    "\n",
    "# The password to the database user\n",
    "password = 'postgres'\n",
    "\n",
    "# The port number for Postgres\n",
    "port = '5432'\n",
    "\n",
    "\n",
    "########################################################\n",
    "#\n",
    "#               Graph semantics\n",
    "#\n",
    "########################################################\n",
    "\n",
    "# The directions of the following edge types need to be reversed\n",
    "edge_reversed = [\n",
    "    \"EVENT_ACCEPT\",\n",
    "    \"EVENT_RECVFROM\",\n",
    "    \"EVENT_RECVMSG\"\n",
    "]\n",
    "\n",
    "# The following edges are the types only considered to construct the\n",
    "# temporal graph for experiments.\n",
    "include_edge_type=[\n",
    "    \"EVENT_WRITE\",\n",
    "    \"EVENT_READ\",\n",
    "    \"EVENT_CLOSE\",\n",
    "    \"EVENT_OPEN\",\n",
    "    \"EVENT_EXECUTE\",\n",
    "    \"EVENT_SENDTO\",\n",
    "    \"EVENT_RECVFROM\",\n",
    "]\n",
    "\n",
    "# The map between edge type and edge ID\n",
    "rel2id = {\n",
    " 1: 'EVENT_WRITE',\n",
    " 'EVENT_WRITE': 1,\n",
    " 2: 'EVENT_READ',\n",
    " 'EVENT_READ': 2,\n",
    " 3: 'EVENT_CLOSE',\n",
    " 'EVENT_CLOSE': 3,\n",
    " 4: 'EVENT_OPEN',\n",
    " 'EVENT_OPEN': 4,\n",
    " 5: 'EVENT_EXECUTE',\n",
    " 'EVENT_EXECUTE': 5,\n",
    " 6: 'EVENT_SENDTO',\n",
    " 'EVENT_SENDTO': 6,\n",
    " 7: 'EVENT_RECVFROM',\n",
    " 'EVENT_RECVFROM': 7\n",
    "}\n",
    "\n",
    "########################################################\n",
    "#\n",
    "#                   Model dimensionality\n",
    "#\n",
    "########################################################\n",
    "\n",
    "# Node Embedding Dimension\n",
    "node_embedding_dim = 16\n",
    "\n",
    "# Node State Dimension\n",
    "node_state_dim = 100\n",
    "\n",
    "# Neighborhood Sampling Size\n",
    "neighbor_size = 20\n",
    "\n",
    "# Edge Embedding Dimension\n",
    "edge_dim = 100\n",
    "\n",
    "# The time encoding Dimension\n",
    "time_dim = 100\n",
    "\n",
    "\n",
    "########################################################\n",
    "#\n",
    "#                   Train&Test\n",
    "#\n",
    "########################################################\n",
    "\n",
    "# Batch size for training and testing\n",
    "BATCH = 1024\n",
    "\n",
    "# Parameters for optimizer\n",
    "lr=0.00005\n",
    "eps=1e-08\n",
    "weight_decay=0.01\n",
    "\n",
    "epoch_num=50\n",
    "\n",
    "# The size of time window, 60000000000 represent 1 min in nanoseconds.\n",
    "# The default setting is 15 minutes.\n",
    "time_window_size = 60000000000 * 15\n",
    "\n",
    "\n",
    "########################################################\n",
    "#\n",
    "#                   Threshold\n",
    "#\n",
    "########################################################\n",
    "\n",
    "beta_day6 = 100\n",
    "beta_day7 = 100\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Utilities",
   "id": "fe2c596a2ac5b7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pytz\n",
    "from time import mktime\n",
    "from datetime import datetime\n",
    "import time\n",
    "import psycopg2\n",
    "from psycopg2 import extras as ex\n",
    "import os.path as osp\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from torch_geometric.data import TemporalData\n",
    "from torch_geometric.nn import TGNMemory, TransformerConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.models.tgn import (LastNeighborLoader, IdentityMessage, MeanAggregator,\n",
    "                                           LastAggregator)\n",
    "from torch_geometric import *\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import xxhash\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "def ns_time_to_datetime(ns):\n",
    "    \"\"\"\n",
    "    :param ns: int nano timestamp\n",
    "    :return: datetime   format: 2013-10-10 23:40:00.000000000\n",
    "    \"\"\"\n",
    "    dt = datetime.fromtimestamp(int(ns) // 1000000000)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    s += '.' + str(int(int(ns) % 1000000000)).zfill(9)\n",
    "    return s\n",
    "\n",
    "def ns_time_to_datetime_US(ns):\n",
    "    \"\"\"\n",
    "    :param ns: int nano timestamp\n",
    "    :return: datetime   format: 2013-10-10 23:40:00.000000000\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    dt = pytz.datetime.datetime.fromtimestamp(int(ns) // 1000000000, tz)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    s += '.' + str(int(int(ns) % 1000000000)).zfill(9)\n",
    "    return s\n",
    "\n",
    "def time_to_datetime_US(s):\n",
    "    \"\"\"\n",
    "    :param ns: int nano timestamp\n",
    "    :return: datetime   format: 2013-10-10 23:40:00\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    dt = pytz.datetime.datetime.fromtimestamp(int(s), tz)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    return s\n",
    "\n",
    "def datetime_to_ns_time(date):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    timeStamp = int(time.mktime(timeArray))\n",
    "    timeStamp = timeStamp * 1000000000\n",
    "    return timeStamp\n",
    "\n",
    "def datetime_to_ns_time_US(date):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    dt = datetime.fromtimestamp(mktime(timeArray))\n",
    "    timestamp = tz.localize(dt)\n",
    "    timestamp = timestamp.timestamp()\n",
    "    timeStamp = timestamp * 1000000000\n",
    "    return int(timeStamp)\n",
    "\n",
    "def datetime_to_timestamp_US(date):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    dt = datetime.fromtimestamp(mktime(timeArray))\n",
    "    timestamp = tz.localize(dt)\n",
    "    timestamp = timestamp.timestamp()\n",
    "    timeStamp = timestamp\n",
    "    return int(timeStamp)\n",
    "\n",
    "def init_database_connection():\n",
    "    if host is not None:\n",
    "        if host == \"/var/run/postgresql/\":\n",
    "            connect = psycopg2.connect(database = database,\n",
    "                                       host = host,\n",
    "                                       user = user,\n",
    "                                       password = password,\n",
    "                                       port = port\n",
    "                                      )\n",
    "        else:\n",
    "            connect = psycopg2.connect(database=database,\n",
    "                                       host=host,\n",
    "                                       user=user,\n",
    "                                       password=password,\n",
    "                                       port=port,\n",
    "                                       sslmode=\"require\"\n",
    "                                       )\n",
    "    else:\n",
    "        connect = psycopg2.connect(database = database,\n",
    "                                   user = user,\n",
    "                                   password = password,\n",
    "                                   port = port\n",
    "                                  )\n",
    "    cur = connect.cursor()\n",
    "    return cur, connect\n",
    "\n",
    "def gen_nodeid2msg(cur):\n",
    "    sql = \"select * from node2id ORDER BY index_id;\"\n",
    "    cur.execute(sql)\n",
    "    rows = cur.fetchall()\n",
    "    nodeid2msg = {}\n",
    "    for i in rows:\n",
    "        nodeid2msg[i[0]] = i[-1]\n",
    "        nodeid2msg[i[-1]] = {i[1]: i[2]}\n",
    "\n",
    "    return nodeid2msg\n",
    "\n",
    "def tensor_find(t,x):\n",
    "    t_np=t.cpu().numpy()\n",
    "    idx=np.argwhere(t_np==x)\n",
    "    return idx[0][0]+1\n",
    "\n",
    "def std(t):\n",
    "    t = np.array(t)\n",
    "    return np.std(t)\n",
    "\n",
    "def var(t):\n",
    "    t = np.array(t)\n",
    "    return np.var(t)\n",
    "\n",
    "def mean(t):\n",
    "    t = np.array(t)\n",
    "    return np.mean(t)\n",
    "\n",
    "def hashgen(l):\n",
    "    \"\"\"Generate a single hash value from a list. @l is a list of\n",
    "    string values, which can be properties of a node/edge. This\n",
    "    function returns a single hashed integer value.\"\"\"\n",
    "    hasher = xxhash.xxh64()\n",
    "    for e in l:\n",
    "        hasher.update(e)\n",
    "    return hasher.intdigest()"
   ],
   "id": "23176483338e52ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocess",
   "id": "e9aaef5a19906a35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!mkdir -p ./artifact/",
   "id": "7327760a8068a3c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Database",
   "id": "4613dce29a272357"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "\n",
    "filelist = ['ta1-cadets-e3-official.json',\n",
    " 'ta1-cadets-e3-official.json.1',\n",
    " 'ta1-cadets-e3-official.json.2',\n",
    " 'ta1-cadets-e3-official-1.json',\n",
    " 'ta1-cadets-e3-official-1.json.1',\n",
    " 'ta1-cadets-e3-official-1.json.2',\n",
    " 'ta1-cadets-e3-official-1.json.3',\n",
    " 'ta1-cadets-e3-official-1.json.4',\n",
    " 'ta1-cadets-e3-official-2.json',\n",
    " 'ta1-cadets-e3-official-2.json.1']\n",
    "\n",
    "\n",
    "def stringtomd5(originstr):\n",
    "    originstr = originstr.encode(\"utf-8\")\n",
    "    signaturemd5 = hashlib.sha256()\n",
    "    signaturemd5.update(originstr)\n",
    "    return signaturemd5.hexdigest()\n",
    "\n",
    "def store_netflow(file_path, cur, connect):\n",
    "    # Parse data from logs\n",
    "    netobjset = set()\n",
    "    netobj2hash = {}\n",
    "    for file in tqdm(filelist):\n",
    "        with open(file_path + file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if \"NetFlowObject\" in line:\n",
    "                    try:\n",
    "                        res = re.findall(\n",
    "                            'NetFlowObject\":{\"uuid\":\"(.*?)\"(.*?)\"localAddress\":\"(.*?)\",\"localPort\":(.*?),\"remoteAddress\":\"(.*?)\",\"remotePort\":(.*?),',\n",
    "                            line)[0]\n",
    "\n",
    "                        nodeid = res[0]\n",
    "                        srcaddr = res[2]\n",
    "                        srcport = res[3]\n",
    "                        dstaddr = res[4]\n",
    "                        dstport = res[5]\n",
    "\n",
    "                        nodeproperty = srcaddr + \",\" + srcport + \",\" + dstaddr + \",\" + dstport\n",
    "                        hashstr = stringtomd5(nodeproperty)\n",
    "                        netobj2hash[nodeid] = [hashstr, nodeproperty]\n",
    "                        netobj2hash[hashstr] = nodeid\n",
    "                        netobjset.add(hashstr)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "    # Store data into database\n",
    "    datalist = []\n",
    "    for i in netobj2hash.keys():\n",
    "        if len(i) != 64:\n",
    "            datalist.append([i] + [netobj2hash[i][0]] + netobj2hash[i][1].split(\",\"))\n",
    "\n",
    "    sql = '''insert into netflow_node_table\n",
    "                         values %s\n",
    "            '''\n",
    "    ex.execute_values(cur, sql, datalist, page_size=10000)\n",
    "    connect.commit()\n",
    "\n",
    "def store_subject(file_path, cur, connect):\n",
    "    # Parse data from logs\n",
    "    scusess_count = 0\n",
    "    fail_count = 0\n",
    "    subject_objset = set()\n",
    "    subject_obj2hash = {}  #\n",
    "    for file in tqdm(filelist):\n",
    "        with open(file_path + file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if \"Event\" in line:\n",
    "                    subject_uuid = re.findall(\n",
    "                        '\"subject\":{\"com.bbn.tc.schema.avro.cdm18.UUID\":\"(.*?)\"}(.*?)\"exec\":\"(.*?)\"', line)\n",
    "                    try:\n",
    "                        subject_obj2hash[subject_uuid[0][0]] = subject_uuid[0][-1]\n",
    "                        scusess_count += 1\n",
    "                    except:\n",
    "                        try:\n",
    "                            subject_obj2hash[subject_uuid[0][0]] = \"null\"\n",
    "                        except:\n",
    "                            pass\n",
    "                        fail_count += 1\n",
    "    # Store into database\n",
    "    datalist = []\n",
    "    for i in subject_obj2hash.keys():\n",
    "        if len(i) != 64:\n",
    "            datalist.append([i] + [stringtomd5(subject_obj2hash[i]), subject_obj2hash[i]])\n",
    "    sql = '''insert into subject_node_table\n",
    "                         values %s\n",
    "            '''\n",
    "    ex.execute_values(cur, sql, datalist, page_size=10000)\n",
    "    connect.commit()\n",
    "\n",
    "def store_file(file_path, cur, connect):\n",
    "    file_node = set()\n",
    "    for file in tqdm(filelist):\n",
    "        with open(file_path + file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if \"com.bbn.tc.schema.avro.cdm18.FileObject\" in line:\n",
    "                    Object_uuid = re.findall('FileObject\":{\"uuid\":\"(.*?)\",', line)\n",
    "                    try:\n",
    "                        file_node.add(Object_uuid[0])\n",
    "                    except:\n",
    "                        print(line)\n",
    "\n",
    "    file_obj2hash = {}\n",
    "    for file in tqdm(filelist):\n",
    "        with open(file_path + file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if '{\"datum\":{\"com.bbn.tc.schema.avro.cdm18.Event\"' in line:\n",
    "                    predicateObject_uuid = re.findall('\"predicateObject\":{\"com.bbn.tc.schema.avro.cdm18.UUID\":\"(.*?)\"}',\n",
    "                                                      line)\n",
    "                    if len(predicateObject_uuid) > 0:\n",
    "                        if predicateObject_uuid[0] in file_node:\n",
    "                            if '\"predicateObjectPath\":null,' not in line and '<unknown>' not in line:\n",
    "                                path_name = re.findall('\"predicateObjectPath\":{\"string\":\"(.*?)\"', line)\n",
    "                                file_obj2hash[predicateObject_uuid[0]] = path_name\n",
    "\n",
    "    datalist = []\n",
    "    for i in file_obj2hash.keys():\n",
    "        if len(i) != 64:\n",
    "            datalist.append([i] + [stringtomd5(file_obj2hash[i][0]), file_obj2hash[i][0]])\n",
    "    sql = '''insert into file_node_table\n",
    "                         values %s\n",
    "            '''\n",
    "    ex.execute_values(cur, sql, datalist, page_size=10000)\n",
    "    connect.commit()\n",
    "\n",
    "def create_node_list(cur, connect):\n",
    "    node_list = {}\n",
    "\n",
    "    # file\n",
    "    sql = \"\"\"\n",
    "    select * from file_node_table;\n",
    "    \"\"\"\n",
    "    cur.execute(sql)\n",
    "    records = cur.fetchall()\n",
    "\n",
    "    for i in records:\n",
    "        node_list[i[1]] = [\"file\", i[-1]]\n",
    "    file_uuid2hash = {}\n",
    "    for i in records:\n",
    "        file_uuid2hash[i[0]] = i[1]\n",
    "\n",
    "    # subject\n",
    "    sql = \"\"\"\n",
    "    select * from subject_node_table;\n",
    "    \"\"\"\n",
    "    cur.execute(sql)\n",
    "    records = cur.fetchall()\n",
    "    for i in records:\n",
    "        node_list[i[1]] = [\"subject\", i[-1]]\n",
    "    subject_uuid2hash = {}\n",
    "    for i in records:\n",
    "        subject_uuid2hash[i[0]] = i[1]\n",
    "\n",
    "    # netflow\n",
    "    sql = \"\"\"\n",
    "    select * from netflow_node_table;\n",
    "    \"\"\"\n",
    "    cur.execute(sql)\n",
    "    records = cur.fetchall()\n",
    "    for i in records:\n",
    "        node_list[i[1]] = [\"netflow\", i[-2] + \":\" + i[-1]]\n",
    "\n",
    "    net_uuid2hash = {}\n",
    "    for i in records:\n",
    "        net_uuid2hash[i[0]] = i[1]\n",
    "\n",
    "    node_list_database = []\n",
    "    node_index = 0\n",
    "    for i in node_list:\n",
    "        node_list_database.append([i] + node_list[i] + [node_index])\n",
    "        node_index += 1\n",
    "\n",
    "    sql = '''insert into node2id\n",
    "                         values %s\n",
    "            '''\n",
    "    ex.execute_values(cur, sql, node_list_database, page_size=10000)\n",
    "    connect.commit()\n",
    "\n",
    "    sql = \"select * from node2id ORDER BY index_id;\"\n",
    "    cur.execute(sql)\n",
    "    rows = cur.fetchall()\n",
    "    nodeid2msg = {}\n",
    "    for i in rows:\n",
    "        nodeid2msg[i[0]] = i[-1]\n",
    "        nodeid2msg[i[-1]] = {i[1]: i[2]}\n",
    "\n",
    "    return nodeid2msg, subject_uuid2hash, file_uuid2hash, net_uuid2hash\n",
    "\n",
    "def store_event(file_path, cur, connect, reverse, nodeid2msg, subject_uuid2hash, file_uuid2hash, net_uuid2hash):\n",
    "    datalist = []\n",
    "    for file in tqdm(filelist):\n",
    "        with open(file_path + file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if '{\"datum\":{\"com.bbn.tc.schema.avro.cdm18.Event\"' in line and \"EVENT_FLOWS_TO\" not in line:\n",
    "                    subject_uuid = re.findall('\"subject\":{\"com.bbn.tc.schema.avro.cdm18.UUID\":\"(.*?)\"}', line)\n",
    "                    predicateObject_uuid = re.findall('\"predicateObject\":{\"com.bbn.tc.schema.avro.cdm18.UUID\":\"(.*?)\"}', line)\n",
    "                    if len(subject_uuid) > 0 and len(predicateObject_uuid) > 0:\n",
    "                        if subject_uuid[0] in subject_uuid2hash and (predicateObject_uuid[0] in file_uuid2hash or predicateObject_uuid[0] in net_uuid2hash):\n",
    "                            relation_type = re.findall('\"type\":\"(.*?)\"', line)[0]\n",
    "                            time_rec = re.findall('\"timestampNanos\":(.*?),', line)[0]\n",
    "                            time_rec = int(time_rec)\n",
    "                            subjectId = subject_uuid2hash[subject_uuid[0]]\n",
    "                            if predicateObject_uuid[0] in file_uuid2hash:\n",
    "                                objectId = file_uuid2hash[predicateObject_uuid[0]]\n",
    "                            else:\n",
    "                                objectId = net_uuid2hash[predicateObject_uuid[0]]\n",
    "                            if relation_type in reverse:\n",
    "                                datalist.append(\n",
    "                                    [objectId, nodeid2msg[objectId], relation_type, subjectId, nodeid2msg[subjectId],\n",
    "                                     time_rec])\n",
    "                            else:\n",
    "                                datalist.append(\n",
    "                                    [subjectId, nodeid2msg[subjectId], relation_type, objectId, nodeid2msg[objectId],\n",
    "                                     time_rec])\n",
    "\n",
    "    sql = '''insert into event_table\n",
    "                         values %s\n",
    "            '''\n",
    "    ex.execute_values(cur, sql, datalist, page_size=10000)\n",
    "    connect.commit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cur, connect = init_database_connection()\n",
    "\n",
    "    # There will be 155322 netflow nodes stored in the table\n",
    "    print(\"Processing netflow data\")\n",
    "    store_netflow(file_path=raw_dir, cur=cur, connect=connect)\n",
    "\n",
    "    # There will be 224146 subject nodes stored in the table\n",
    "    print(\"Processing subject data\")\n",
    "    store_subject(file_path=raw_dir, cur=cur, connect=connect)\n",
    "\n",
    "    # There will be 234245 file nodes stored in the table\n",
    "    print(\"Processing file data\")\n",
    "    store_file(file_path=raw_dir, cur=cur, connect=connect)\n",
    "\n",
    "    # There will be 268242 entities stored in the table\n",
    "    print(\"Extracting the node list\")\n",
    "    nodeid2msg, subject_uuid2hash, file_uuid2hash, net_uuid2hash = create_node_list(cur=cur, connect=connect)\n",
    "\n",
    "    # There will be 29727441 events stored in the table\n",
    "    print(\"Processing the events\")\n",
    "    store_event(\n",
    "        file_path=raw_dir,\n",
    "        cur=cur,\n",
    "        connect=connect,\n",
    "        reverse=edge_reversed,\n",
    "        nodeid2msg=nodeid2msg,\n",
    "        subject_uuid2hash=subject_uuid2hash,\n",
    "        file_uuid2hash=file_uuid2hash,\n",
    "        net_uuid2hash=net_uuid2hash\n",
    "    )"
   ],
   "id": "f6b94f404e76717f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Embed graph",
   "id": "6b2dc2152cb9c03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from torch_geometric.data import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import logging\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Setting for logging\n",
    "logger = logging.getLogger(\"embedding_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "file_handler = logging.FileHandler(artifact_dir + 'embedding.log')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "def path2higlist(p):\n",
    "    l=[]\n",
    "    spl=p.strip().split('/')\n",
    "    for i in spl:\n",
    "        if len(l)!=0:\n",
    "            l.append(l[-1]+'/'+i)\n",
    "        else:\n",
    "            l.append(i)\n",
    "    return l\n",
    "\n",
    "def ip2higlist(p):\n",
    "    l=[]\n",
    "    spl=p.strip().split('.')\n",
    "    for i in spl:\n",
    "        if len(l)!=0:\n",
    "            l.append(l[-1]+'.'+i)\n",
    "        else:\n",
    "            l.append(i)\n",
    "    return l\n",
    "\n",
    "def list2str(l):\n",
    "    s=''\n",
    "    for i in l:\n",
    "        s+=i\n",
    "    return s\n",
    "\n",
    "def gen_feature(cur):\n",
    "    # Firstly obtain all node labels\n",
    "    nodeid2msg = gen_nodeid2msg(cur=cur)\n",
    "\n",
    "    # Construct the hierarchical representation for each node label\n",
    "    node_msg_dic_list = []\n",
    "    for i in tqdm(nodeid2msg.keys()):\n",
    "        if type(i) == int:\n",
    "            if 'netflow' in nodeid2msg[i].keys():\n",
    "                higlist = ['netflow']\n",
    "                higlist += ip2higlist(nodeid2msg[i]['netflow'])\n",
    "\n",
    "            if 'file' in nodeid2msg[i].keys():\n",
    "                higlist = ['file']\n",
    "                higlist += path2higlist(nodeid2msg[i]['file'])\n",
    "\n",
    "            if 'subject' in nodeid2msg[i].keys():\n",
    "                higlist = ['subject']\n",
    "                higlist += path2higlist(nodeid2msg[i]['subject'])\n",
    "            node_msg_dic_list.append(higlist)\n",
    "\n",
    "    # Featurize the hierarchical node labels\n",
    "    FH_string = FeatureHasher(n_features=node_embedding_dim, input_type=\"string\")\n",
    "    node2higvec=[]\n",
    "    for i in tqdm(node_msg_dic_list):\n",
    "        vec=FH_string.transform([i]).toarray()\n",
    "        node2higvec.append(vec)\n",
    "    node2higvec = np.array(node2higvec).reshape([-1, node_embedding_dim])\n",
    "    torch.save(node2higvec, artifact_dir + \"node2higvec\")\n",
    "    return node2higvec\n",
    "\n",
    "def gen_relation_onehot():\n",
    "    relvec=torch.nn.functional.one_hot(torch.arange(0, len(rel2id.keys())//2), num_classes=len(rel2id.keys())//2)\n",
    "    rel2vec={}\n",
    "    for i in rel2id.keys():\n",
    "        if type(i) is not int:\n",
    "            rel2vec[i]= relvec[rel2id[i]-1]\n",
    "            rel2vec[relvec[rel2id[i]-1]]=i\n",
    "    torch.save(rel2vec, artifact_dir + \"rel2vec\")\n",
    "    return rel2vec\n",
    "\n",
    "def gen_vectorized_graphs(cur, node2higvec, rel2vec, logger):\n",
    "    for day in tqdm(range(2, 14)):\n",
    "        start_timestamp = datetime_to_ns_time_US('2018-04-' + str(day) + ' 00:00:00')\n",
    "        end_timestamp = datetime_to_ns_time_US('2018-04-' + str(day + 1) + ' 00:00:00')\n",
    "        sql = \"\"\"\n",
    "        select * from event_table\n",
    "        where\n",
    "              timestamp_rec>'%s' and timestamp_rec<'%s'\n",
    "               ORDER BY timestamp_rec;\n",
    "        \"\"\" % (start_timestamp, end_timestamp)\n",
    "        cur.execute(sql)\n",
    "        events = cur.fetchall()\n",
    "        logger.info(f'2018-04-{day}, events count: {len(events)}')\n",
    "        edge_list = []\n",
    "        for e in events:\n",
    "            edge_temp = [int(e[1]), int(e[4]), e[2], e[5]]\n",
    "            if e[2] in include_edge_type:\n",
    "                edge_list.append(edge_temp)\n",
    "        logger.info(f'2018-04-{day}, edge list len: {len(edge_list)}')\n",
    "        dataset = TemporalData()\n",
    "        src = []\n",
    "        dst = []\n",
    "        msg = []\n",
    "        t = []\n",
    "        for i in edge_list:\n",
    "            src.append(int(i[0]))\n",
    "            dst.append(int(i[1]))\n",
    "            msg.append(\n",
    "                torch.cat([torch.from_numpy(node2higvec[i[0]]), rel2vec[i[2]], torch.from_numpy(node2higvec[i[1]])]))\n",
    "            t.append(int(i[3]))\n",
    "\n",
    "        dataset.src = torch.tensor(src)\n",
    "        dataset.dst = torch.tensor(dst)\n",
    "        dataset.t = torch.tensor(t)\n",
    "        dataset.msg = torch.vstack(msg)\n",
    "        dataset.src = dataset.src.to(torch.long)\n",
    "        dataset.dst = dataset.dst.to(torch.long)\n",
    "        dataset.msg = dataset.msg.to(torch.float)\n",
    "        dataset.t = dataset.t.to(torch.long)\n",
    "        torch.save(dataset, graphs_dir + \"/graph_4_\" + str(day) + \".TemporalData.simple\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Start logging.\")\n",
    "\n",
    "    os.system(f\"mkdir -p {graphs_dir}\")\n",
    "\n",
    "    cur, _ = init_database_connection()\n",
    "    node2higvec = gen_feature(cur=cur)\n",
    "    rel2vec = gen_relation_onehot()\n",
    "    gen_vectorized_graphs(cur=cur, node2higvec=node2higvec, rel2vec=rel2vec, logger=logger)\n",
    "\n"
   ],
   "id": "5758ed8b403d6b87"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Graph learning",
   "id": "e5511813be4ffc72"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "from torch_geometric.nn import TGNMemory, TransformerConv, SAGEConv\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "max_node_num = 268243  # the number of nodes in node2id table +1\n",
    "min_dst_idx, max_dst_idx = 0, max_node_num\n",
    "# Helper vector to map global node indices to local ones.\n",
    "assoc = torch.empty(max_node_num, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "\n",
    "class GraphAttentionEmbedding(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, msg_dim, time_enc):\n",
    "        super(GraphAttentionEmbedding, self).__init__()\n",
    "        self.time_enc = time_enc\n",
    "        edge_dim = msg_dim + time_enc.out_channels\n",
    "        self.conv = TransformerConv(in_channels, out_channels, heads=8,\n",
    "                                    dropout=0.0, edge_dim=edge_dim)\n",
    "        self.conv2 = TransformerConv(out_channels * 8, out_channels, heads=1, concat=False,\n",
    "                                     dropout=0.0, edge_dim=edge_dim)\n",
    "\n",
    "    def forward(self, x, last_update, edge_index, t, msg):\n",
    "        last_update.to(device)\n",
    "        x = x.to(device)\n",
    "        t = t.to(device)\n",
    "        rel_t = last_update[edge_index[0]] - t\n",
    "        rel_t_enc = self.time_enc(rel_t.to(x.dtype))\n",
    "        edge_attr = torch.cat([rel_t_enc, msg], dim=-1)\n",
    "        x = F.relu(self.conv(x, edge_index, edge_attr))\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GraphSAGEEmbedding(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, msg_dim, time_enc):\n",
    "        super(GraphSAGEEmbedding, self).__init__()\n",
    "\n",
    "\n",
    "        self.time_enc = time_enc\n",
    "\n",
    "\n",
    "        self.conv1 = SAGEConv(in_channels, out_channels * 8, aggr=\"mean\")\n",
    "        self.conv2 = SAGEConv(out_channels * 8, out_channels, aggr=\"mean\")\n",
    "\n",
    "    def forward(self, x, last_update, edge_index, t, msg):\n",
    "\n",
    "        x = x.to(device)\n",
    "        edge_index = edge_index.to(device)\n",
    "\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "class LinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(LinkPredictor, self).__init__()\n",
    "        self.lin_src = Linear(in_channels, in_channels * 2)\n",
    "        self.lin_dst = Linear(in_channels, in_channels * 2)\n",
    "\n",
    "        self.lin_seq = nn.Sequential(\n",
    "\n",
    "            Linear(in_channels * 4, in_channels * 8),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            nn.Tanh(),\n",
    "            Linear(in_channels * 8, in_channels * 2),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            nn.Tanh(),\n",
    "            Linear(in_channels * 2, int(in_channels // 2)),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            nn.Tanh(),\n",
    "            Linear(int(in_channels // 2), out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, z_src, z_dst):\n",
    "        h = torch.cat([self.lin_src(z_src), self.lin_dst(z_dst)], dim=-1)\n",
    "        h = self.lin_seq(h)\n",
    "        return h\n",
    "\n",
    "def cal_pos_edges_loss_multiclass(link_pred_ratio,labels):\n",
    "    loss=[]\n",
    "    for i in range(len(link_pred_ratio)):\n",
    "        loss.append(criterion(link_pred_ratio[i].reshape(1,-1),labels[i].reshape(-1)))\n",
    "    return torch.tensor(loss)\n"
   ],
   "id": "8a117c876b492941"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train",
   "id": "30c450f914e32fdc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "##########################################################################################\n",
    "# Some of the code is adapted from:\n",
    "# https://github.com/pyg-team/pytorch_geometric/blob/master/examples/tgn.py\n",
    "##########################################################################################\n",
    "\n",
    "import logging\n",
    "\n",
    "from torch_geometric.loader import TemporalDataLoader\n",
    "\n",
    "# Setting for logging\n",
    "logger = logging.getLogger(\"training_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "file_handler = logging.FileHandler(artifact_dir + 'training.log')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "\n",
    "def train(train_data,\n",
    "          memory,\n",
    "          gnn,\n",
    "          link_pred,\n",
    "          optimizer,\n",
    "          neighbor_loader\n",
    "          ):\n",
    "    memory.train()\n",
    "    gnn.train()\n",
    "    link_pred.train()\n",
    "\n",
    "    memory.reset_state()  # Start with a fresh memory.\n",
    "    neighbor_loader.reset_state()  # Start with an empty graph.\n",
    "\n",
    "    total_loss = 0\n",
    "    loader = TemporalDataLoader(train_data, batch_size=BATCH)\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        src, pos_dst, t, msg = batch.src, batch.dst, batch.t, batch.msg\n",
    "\n",
    "        n_id = torch.cat([src, pos_dst]).unique()\n",
    "        n_id, edge_index, e_id = neighbor_loader(n_id)\n",
    "        assoc[n_id] = torch.arange(n_id.size(0), device=device)\n",
    "\n",
    "        # Get updated memory of all nodes involved in the computation.\n",
    "        z, last_update = memory(n_id)\n",
    "        z = gnn(z, last_update, edge_index, train_data.t[e_id], train_data.msg[e_id])\n",
    "        pos_out = link_pred(z[assoc[src]], z[assoc[pos_dst]])\n",
    "\n",
    "        y_pred = torch.cat([pos_out], dim=0)\n",
    "        y_true = []\n",
    "        for m in msg:\n",
    "            l = tensor_find(m[node_embedding_dim:-node_embedding_dim], 1) - 1\n",
    "            y_true.append(l)\n",
    "\n",
    "        y_true = torch.tensor(y_true).to(device=device)\n",
    "        y_true = y_true.reshape(-1).to(torch.long).to(device=device)\n",
    "\n",
    "        loss = criterion(y_pred, y_true)\n",
    "\n",
    "        # Update memory and neighbor loader with ground-truth state.\n",
    "        memory.update_state(src, pos_dst, t, msg)\n",
    "        neighbor_loader.insert(src, pos_dst)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        memory.detach()\n",
    "        total_loss += float(loss) * batch.num_events\n",
    "    return total_loss / train_data.num_events\n",
    "\n",
    "def load_train_data():\n",
    "    graph_4_2 = torch.load(graphs_dir + \"/graph_4_2.TemporalData.simple\", weights_only=False).to(device=device)\n",
    "    graph_4_3 = torch.load(graphs_dir + \"/graph_4_3.TemporalData.simple\", weights_only=False).to(device=device)\n",
    "    graph_4_4 = torch.load(graphs_dir + \"/graph_4_4.TemporalData.simple\", weights_only=False).to(device=device)\n",
    "    return [graph_4_2, graph_4_3, graph_4_4]\n",
    "\n",
    "def init_models(node_feat_size):\n",
    "    memory = TGNMemory(\n",
    "        max_node_num,\n",
    "        node_feat_size,\n",
    "        node_state_dim,\n",
    "        time_dim,\n",
    "        message_module=IdentityMessage(node_feat_size, node_state_dim, time_dim),\n",
    "        aggregator_module=LastAggregator(),\n",
    "    ).to(device)\n",
    "\n",
    "    gnn = GraphAttentionEmbedding(\n",
    "        in_channels=node_state_dim,\n",
    "        out_channels=edge_dim,\n",
    "        msg_dim=node_feat_size,\n",
    "        time_enc=memory.time_enc,\n",
    "    ).to(device)\n",
    "\n",
    "    out_channels = len(include_edge_type)\n",
    "    link_pred = LinkPredictor(in_channels=edge_dim, out_channels=out_channels).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        set(memory.parameters()) | set(gnn.parameters())\n",
    "        | set(link_pred.parameters()), lr=lr, eps=eps, weight_decay=weight_decay)\n",
    "\n",
    "    neighbor_loader = LastNeighborLoader(max_node_num, size=neighbor_size, device=device)\n",
    "\n",
    "    return memory, gnn, link_pred, optimizer, neighbor_loader\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Start logging.\")\n",
    "\n",
    "    # Load data for training\n",
    "    train_data = load_train_data()\n",
    "\n",
    "    # Initialize the models and the optimizer\n",
    "    node_feat_size = train_data[0].msg.size(-1)\n",
    "    memory, gnn, link_pred, optimizer, neighbor_loader = init_models(node_feat_size=node_feat_size)\n",
    "\n",
    "    # train the model\n",
    "    for epoch in tqdm(range(1, epoch_num+1)):\n",
    "        for g in train_data:\n",
    "            loss = train(\n",
    "                train_data=g,\n",
    "                memory=memory,\n",
    "                gnn=gnn,\n",
    "                link_pred=link_pred,\n",
    "                optimizer=optimizer,\n",
    "                neighbor_loader=neighbor_loader\n",
    "            )\n",
    "            logger.info(f'  Epoch: {epoch:02d}, Loss: {loss:.4f}')\n",
    "\n",
    "    # Save the trained model\n",
    "    model = [memory, gnn, link_pred, neighbor_loader]\n",
    "\n",
    "    os.system(f\"mkdir -p {models_dir}\")\n",
    "    torch.save(model, f\"{models_dir}/models.pt\")\n"
   ],
   "id": "b346db237e86d3cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test",
   "id": "c7b34671f110845f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "##########################################################################################\n",
    "# Some of the code is adapted from:\n",
    "# https://github.com/pyg-team/pytorch_geometric/blob/master/examples/tgn.py\n",
    "##########################################################################################\n",
    "\n",
    "import logging\n",
    "\n",
    "from torch_geometric.loader import TemporalDataLoader\n",
    "\n",
    "# Setting for logging\n",
    "logger = logging.getLogger(\"reconstruction_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "file_handler = logging.FileHandler(artifact_dir + 'reconstruction.log')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(inference_data,\n",
    "          memory,\n",
    "          gnn,\n",
    "          link_pred,\n",
    "          neighbor_loader,\n",
    "          nodeid2msg,\n",
    "          path\n",
    "          ):\n",
    "    if os.path.exists(path):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(path)\n",
    "\n",
    "    memory.eval()\n",
    "    gnn.eval()\n",
    "    link_pred.eval()\n",
    "\n",
    "    memory.reset_state()  # Start with a fresh memory.\n",
    "    neighbor_loader.reset_state()  # Start with an empty graph.\n",
    "\n",
    "    time_with_loss = {}  # key: time，  value： the losses\n",
    "    total_loss = 0\n",
    "    edge_list = []\n",
    "\n",
    "    unique_nodes = torch.tensor([]).to(device=device)\n",
    "    total_edges = 0\n",
    "\n",
    "\n",
    "    start_time = inference_data.t[0]\n",
    "    event_count = 0\n",
    "    pos_o = []\n",
    "\n",
    "    # Record the running time to evaluate the performance\n",
    "    start = time.perf_counter()\n",
    "    loader = TemporalDataLoader(inference_data, batch_size=BATCH)\n",
    "    for batch in loader:\n",
    "\n",
    "        src, pos_dst, t, msg = batch.src, batch.dst, batch.t, batch.msg\n",
    "        unique_nodes = torch.cat([unique_nodes, src, pos_dst]).unique()\n",
    "        total_edges += BATCH\n",
    "\n",
    "        n_id = torch.cat([src, pos_dst]).unique()\n",
    "        n_id, edge_index, e_id = neighbor_loader(n_id)\n",
    "        assoc[n_id] = torch.arange(n_id.size(0), device=device)\n",
    "\n",
    "        z, last_update = memory(n_id)\n",
    "        z = gnn(z, last_update, edge_index, inference_data.t[e_id], inference_data.msg[e_id])\n",
    "\n",
    "        pos_out = link_pred(z[assoc[src]], z[assoc[pos_dst]])\n",
    "\n",
    "        pos_o.append(pos_out)\n",
    "        y_pred = torch.cat([pos_out], dim=0)\n",
    "        y_true = []\n",
    "        for m in msg:\n",
    "            l = tensor_find(m[node_embedding_dim:-node_embedding_dim], 1) - 1\n",
    "            y_true.append(l)\n",
    "        y_true = torch.tensor(y_true).to(device=device)\n",
    "        y_true = y_true.reshape(-1).to(torch.long).to(device=device)\n",
    "\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        total_loss += float(loss) * batch.num_events\n",
    "\n",
    "        # update the edges in the batch to the memory and neighbor_loader\n",
    "        memory.update_state(src, pos_dst, t, msg)\n",
    "        neighbor_loader.insert(src, pos_dst)\n",
    "\n",
    "        # compute the loss for each edge\n",
    "        each_edge_loss = cal_pos_edges_loss_multiclass(pos_out, y_true)\n",
    "\n",
    "        for i in range(len(pos_out)):\n",
    "            srcnode = int(src[i])\n",
    "            dstnode = int(pos_dst[i])\n",
    "\n",
    "            srcmsg = str(nodeid2msg[srcnode])\n",
    "            dstmsg = str(nodeid2msg[dstnode])\n",
    "            t_var = int(t[i])\n",
    "            edgeindex = tensor_find(msg[i][node_embedding_dim:-node_embedding_dim], 1)\n",
    "            edge_type = rel2id[edgeindex]\n",
    "            loss = each_edge_loss[i]\n",
    "\n",
    "            temp_dic = {}\n",
    "            temp_dic['loss'] = float(loss)\n",
    "            temp_dic['srcnode'] = srcnode\n",
    "            temp_dic['dstnode'] = dstnode\n",
    "            temp_dic['srcmsg'] = srcmsg\n",
    "            temp_dic['dstmsg'] = dstmsg\n",
    "            temp_dic['edge_type'] = edge_type\n",
    "            temp_dic['time'] = t_var\n",
    "\n",
    "            edge_list.append(temp_dic)\n",
    "\n",
    "        event_count += len(batch.src)\n",
    "        if t[-1] > start_time + time_window_size:\n",
    "            # Here is a checkpoint, which records all edge losses in the current time window\n",
    "            time_interval = ns_time_to_datetime_US(start_time) + \"~\" + ns_time_to_datetime_US(t[-1])\n",
    "\n",
    "            end = time.perf_counter()\n",
    "            time_with_loss[time_interval] = {'loss': loss,\n",
    "\n",
    "                                             'nodes_count': len(unique_nodes),\n",
    "                                             'total_edges': total_edges,\n",
    "                                             'costed_time': (end - start)}\n",
    "\n",
    "            log = open(path + \"/\" + time_interval + \".txt\", 'w')\n",
    "\n",
    "            for e in edge_list:\n",
    "                loss += e['loss']\n",
    "\n",
    "            loss = loss / event_count\n",
    "            logger.info(\n",
    "                f'Time: {time_interval}, Loss: {loss:.4f}, Nodes_count: {len(unique_nodes)}, Edges_count: {event_count}, Cost Time: {(end - start):.2f}s')\n",
    "            edge_list = sorted(edge_list, key=lambda x: x['loss'], reverse=True)  # Rank the results based on edge losses\n",
    "            for e in edge_list:\n",
    "                log.write(str(e))\n",
    "                log.write(\"\\n\")\n",
    "            event_count = 0\n",
    "            total_loss = 0\n",
    "            start_time = t[-1]\n",
    "            log.close()\n",
    "            edge_list.clear()\n",
    "\n",
    "    return time_with_loss\n",
    "\n",
    "def load_data():\n",
    "    # graph_4_3 - graph_4_5 will be used to initialize node IDF scores.\n",
    "    graph_4_3 = torch.load(graphs_dir + \"/graph_4_3.TemporalData.simple\", weights_only=False).to(device=device)\n",
    "    graph_4_4 = torch.load(graphs_dir + \"/graph_4_4.TemporalData.simple\", weights_only=False).to(device=device)\n",
    "    graph_4_5 = torch.load(graphs_dir + \"/graph_4_5.TemporalData.simple\", weights_only=False).to(device=device)\n",
    "\n",
    "    # Testing set\n",
    "    graph_4_6 = torch.load(graphs_dir + \"/graph_4_6.TemporalData.simple\", weights_only=False).to(device=device)\n",
    "    graph_4_7 = torch.load(graphs_dir + \"/graph_4_7.TemporalData.simple\", weights_only=False).to(device=device)\n",
    "\n",
    "    return [graph_4_3, graph_4_4, graph_4_5, graph_4_6, graph_4_7]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Start logging.\")\n",
    "\n",
    "    # load the map between nodeID and node labels\n",
    "    cur, _ = init_database_connection()\n",
    "    nodeid2msg = gen_nodeid2msg(cur=cur)\n",
    "\n",
    "    # Load data\n",
    "    graph_4_3, graph_4_4, graph_4_5, graph_4_6, graph_4_7 = load_data()\n",
    "\n",
    "    # load trained model\n",
    "    memory, gnn, link_pred, neighbor_loader = torch.load(f\"{models_dir}/models.pt\",map_location=device, weights_only=False)\n",
    "\n",
    "    # Reconstruct the edges in each day\n",
    "    test(inference_data=graph_4_3,\n",
    "         memory=memory,\n",
    "         gnn=gnn,\n",
    "         link_pred=link_pred,\n",
    "         neighbor_loader=neighbor_loader,\n",
    "         nodeid2msg=nodeid2msg,\n",
    "         path=artifact_dir + \"graph_4_3\")\n",
    "\n",
    "    test(inference_data=graph_4_4,\n",
    "         memory=memory,\n",
    "         gnn=gnn,\n",
    "         link_pred=link_pred,\n",
    "         neighbor_loader=neighbor_loader,\n",
    "         nodeid2msg=nodeid2msg,\n",
    "         path=artifact_dir + \"graph_4_4\")\n",
    "\n",
    "    test(inference_data=graph_4_5,\n",
    "         memory=memory,\n",
    "         gnn=gnn,\n",
    "         link_pred=link_pred,\n",
    "         neighbor_loader=neighbor_loader,\n",
    "         nodeid2msg=nodeid2msg,\n",
    "         path=artifact_dir + \"graph_4_5\")\n",
    "\n",
    "    test(inference_data=graph_4_6,\n",
    "         memory=memory,\n",
    "         gnn=gnn,\n",
    "         link_pred=link_pred,\n",
    "         neighbor_loader=neighbor_loader,\n",
    "         nodeid2msg=nodeid2msg,\n",
    "         path=artifact_dir + \"graph_4_6\")\n",
    "\n",
    "    test(inference_data=graph_4_7,\n",
    "         memory=memory,\n",
    "         gnn=gnn,\n",
    "         link_pred=link_pred,\n",
    "         neighbor_loader=neighbor_loader,\n",
    "         nodeid2msg=nodeid2msg,\n",
    "         path=artifact_dir + \"graph_4_7\")\n"
   ],
   "id": "4f857efb22b43659"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Anomaly detection",
   "id": "4c5871c0011d40a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Queue",
   "id": "c1f62be5bf5bc4d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import logging\n",
    "\n",
    "import torch\n",
    "\n",
    "# Setting for logging\n",
    "logger = logging.getLogger(\"anomalous_queue_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "file_handler = logging.FileHandler(artifact_dir + 'anomalous_queue.log')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "\n",
    "def cal_anomaly_loss(loss_list, edge_list):\n",
    "    if len(loss_list) != len(edge_list):\n",
    "        print(\"error!\")\n",
    "        return 0\n",
    "    count = 0\n",
    "    loss_sum = 0\n",
    "    loss_std = std(loss_list)\n",
    "    loss_mean = mean(loss_list)\n",
    "    edge_set = set()\n",
    "    node_set = set()\n",
    "\n",
    "    thr = loss_mean + 1.5 * loss_std\n",
    "\n",
    "    logger.info(f\"thr:{thr}\")\n",
    "\n",
    "    for i in range(len(loss_list)):\n",
    "        if loss_list[i] > thr:\n",
    "            count += 1\n",
    "            src_node = edge_list[i][0]\n",
    "            dst_node = edge_list[i][1]\n",
    "            loss_sum += loss_list[i]\n",
    "\n",
    "            node_set.add(src_node)\n",
    "            node_set.add(dst_node)\n",
    "            edge_set.add(edge_list[i][0] + edge_list[i][1])\n",
    "    return count, loss_sum / count, node_set, edge_set\n",
    "\n",
    "def compute_IDF():\n",
    "    node_IDF = {}\n",
    "\n",
    "    file_list = []\n",
    "    file_path = artifact_dir + \"graph_4_3/\"\n",
    "    file_l = os.listdir(file_path)\n",
    "    for i in file_l:\n",
    "        file_list.append(file_path + i)\n",
    "\n",
    "    file_path = artifact_dir + \"graph_4_4/\"\n",
    "    file_l = os.listdir(file_path)\n",
    "    for i in file_l:\n",
    "        file_list.append(file_path + i)\n",
    "\n",
    "    file_path = artifact_dir + \"graph_4_5/\"\n",
    "    file_l = os.listdir(file_path)\n",
    "    for i in file_l:\n",
    "        file_list.append(file_path + i)\n",
    "\n",
    "    node_set = {}\n",
    "    for f_path in tqdm(file_list):\n",
    "        f = open(f_path)\n",
    "        for line in f:\n",
    "            l = line.strip()\n",
    "            jdata = eval(l)\n",
    "            if jdata['loss'] > 0:\n",
    "                if 'netflow' not in str(jdata['srcmsg']):\n",
    "                    if str(jdata['srcmsg']) not in node_set.keys():\n",
    "                        node_set[str(jdata['srcmsg'])] = {f_path}\n",
    "                    else:\n",
    "                        node_set[str(jdata['srcmsg'])].add(f_path)\n",
    "                if 'netflow' not in str(jdata['dstmsg']):\n",
    "                    if str(jdata['dstmsg']) not in node_set.keys():\n",
    "                        node_set[str(jdata['dstmsg'])] = {f_path}\n",
    "                    else:\n",
    "                        node_set[str(jdata['dstmsg'])].add(f_path)\n",
    "    for n in node_set:\n",
    "        include_count = len(node_set[n])\n",
    "        IDF = math.log(len(file_list) / (include_count + 1))\n",
    "        node_IDF[n] = IDF\n",
    "\n",
    "    torch.save(node_IDF, artifact_dir + \"node_IDF\")\n",
    "    logger.info(\"IDF weight calculate complete!\")\n",
    "    return node_IDF, file_list\n",
    "\n",
    "# Measure the relationship between two time windows, if the returned value\n",
    "# is not 0, it means there are suspicious nodes in both time windows.\n",
    "def cal_set_rel(s1, s2, node_IDF, tw_list):\n",
    "    def is_include_key_word(s):\n",
    "        # The following common nodes don't exist in the training/validation data, but\n",
    "        # will have the influences to the construction of anomalous queue (i.e. noise).\n",
    "        # These nodes frequently exist in the testing data but don't contribute much to\n",
    "        # the detection (including temporary files or files with random name).\n",
    "        # Assume the IDF can keep being updated with the new time windows, these\n",
    "        # common nodes can be filtered out.\n",
    "        keywords = [\n",
    "            'netflow',\n",
    "            '/home/george/Drafts',\n",
    "            'usr',\n",
    "            'proc',\n",
    "            'var',\n",
    "            'cadet',\n",
    "            '/var/log/debug.log',\n",
    "            '/var/log/cron',\n",
    "            '/home/charles/Drafts',\n",
    "            '/etc/ssl/cert.pem',\n",
    "            '/tmp/.31.3022e',\n",
    "        ]\n",
    "        flag = False\n",
    "        for i in keywords:\n",
    "            if i in s:\n",
    "                flag = True\n",
    "        return flag\n",
    "\n",
    "    new_s = s1 & s2\n",
    "    count = 0\n",
    "    for i in new_s:\n",
    "        if is_include_key_word(i) is True:\n",
    "            node_IDF[i] = math.log(len(tw_list) / (1 + len(tw_list)))\n",
    "\n",
    "        if i in node_IDF.keys():\n",
    "            IDF = node_IDF[i]\n",
    "        else:\n",
    "            # Assign a high IDF for those nodes which are neither in training/validation\n",
    "            # sets nor excluded node list above.\n",
    "            IDF = math.log(len(tw_list) / (1))\n",
    "\n",
    "        # Compare the IDF with a rareness threshold α\n",
    "        if IDF > (math.log(len(tw_list) * 0.9)):\n",
    "            logger.info(f\"node:{i}, IDF:{IDF}\")\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def anomalous_queue_construction(node_IDF, tw_list, graph_dir_path):\n",
    "    history_list = []\n",
    "    current_tw = {}\n",
    "\n",
    "    file_l = os.listdir(graph_dir_path)\n",
    "    index_count = 0\n",
    "    for f_path in sorted(file_l):\n",
    "        logger.info(\"**************************************************\")\n",
    "        logger.info(f\"Time window: {f_path}\")\n",
    "\n",
    "        f = open(f\"{graph_dir_path}/{f_path}\")\n",
    "        edge_loss_list = []\n",
    "        edge_list = []\n",
    "        logger.info(f'Time window index: {index_count}')\n",
    "\n",
    "        # Figure out which nodes are anomalous in this time window\n",
    "        for line in f:\n",
    "            l = line.strip()\n",
    "            jdata = eval(l)\n",
    "            edge_loss_list.append(jdata['loss'])\n",
    "            edge_list.append([str(jdata['srcmsg']), str(jdata['dstmsg'])])\n",
    "        count, loss_avg, node_set, edge_set = cal_anomaly_loss(edge_loss_list, edge_list)\n",
    "        current_tw['name'] = f_path\n",
    "        current_tw['loss'] = loss_avg\n",
    "        current_tw['index'] = index_count\n",
    "        current_tw['nodeset'] = node_set\n",
    "\n",
    "        # Incrementally construct the queues\n",
    "        added_que_flag = False\n",
    "        for hq in history_list:\n",
    "            for his_tw in hq:\n",
    "                if cal_set_rel(current_tw['nodeset'], his_tw['nodeset'], node_IDF, tw_list) != 0 and current_tw['name'] != his_tw['name']:\n",
    "                    hq.append(copy.deepcopy(current_tw))\n",
    "                    added_que_flag = True\n",
    "                    break\n",
    "                if added_que_flag:\n",
    "                    break\n",
    "        if added_que_flag is False:\n",
    "            temp_hq = [copy.deepcopy(current_tw)]\n",
    "            history_list.append(temp_hq)\n",
    "\n",
    "        index_count += 1\n",
    "\n",
    "\n",
    "        logger.info(f\"Average loss: {loss_avg}\")\n",
    "        logger.info(f\"Num of anomalous edges within the time window: {count}\")\n",
    "        logger.info(f\"Percentage of anomalous edges: {count / len(edge_list)}\")\n",
    "        logger.info(f\"Anomalous node count: {len(node_set)}\")\n",
    "        logger.info(f\"Anomalous edge count: {len(edge_set)}\")\n",
    "        logger.info(\"**************************************************\")\n",
    "\n",
    "    return history_list\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Start logging.\")\n",
    "\n",
    "    node_IDF, tw_list = compute_IDF()\n",
    "\n",
    "    # Validation date\n",
    "    history_list = anomalous_queue_construction(\n",
    "        node_IDF=node_IDF,\n",
    "        tw_list=tw_list,\n",
    "        graph_dir_path=f\"{artifact_dir}/graph_4_5/\"\n",
    "    )\n",
    "    torch.save(history_list, f\"{artifact_dir}/graph_4_5_history_list\")\n",
    "\n",
    "    # Testing date\n",
    "    history_list = anomalous_queue_construction(\n",
    "        node_IDF=node_IDF,\n",
    "        tw_list=tw_list,\n",
    "        graph_dir_path=f\"{artifact_dir}/graph_4_6/\"\n",
    "    )\n",
    "    torch.save(history_list, f\"{artifact_dir}/graph_4_6_history_list\")\n",
    "\n",
    "    history_list = anomalous_queue_construction(\n",
    "        node_IDF=node_IDF,\n",
    "        tw_list=tw_list,\n",
    "        graph_dir_path=f\"{artifact_dir}/graph_4_7/\"\n",
    "    )\n",
    "    torch.save(history_list, f\"{artifact_dir}/graph_4_7_history_list\")"
   ],
   "id": "5634a2b0596eb91a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluation",
   "id": "8bea597226c1fced"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import logging\n",
    "\n",
    "# Setting for logging\n",
    "logger = logging.getLogger(\"evaluation_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "file_handler = logging.FileHandler(artifact_dir + 'evaluation.log')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "\n",
    "def classifier_evaluation(y_test, y_test_pred):\n",
    "    tn, fp, fn, tp =confusion_matrix(y_test, y_test_pred).ravel()\n",
    "    logger.info(f'tn: {tn}')\n",
    "    logger.info(f'fp: {fp}')\n",
    "    logger.info(f'fn: {fn}')\n",
    "    logger.info(f'tp: {tp}')\n",
    "\n",
    "    precision=tp/(tp+fp)\n",
    "    recall=tp/(tp+fn)\n",
    "    accuracy=(tp+tn)/(tp+tn+fp+fn)\n",
    "    fscore=2*(precision*recall)/(precision+recall)\n",
    "    auc_val=roc_auc_score(y_test, y_test_pred)\n",
    "    logger.info(f\"precision: {precision}\")\n",
    "    logger.info(f\"recall: {recall}\")\n",
    "    logger.info(f\"fscore: {fscore}\")\n",
    "    logger.info(f\"accuracy: {accuracy}\")\n",
    "    logger.info(f\"auc_val: {auc_val}\")\n",
    "    return precision,recall,fscore,accuracy,auc_val\n",
    "\n",
    "def ground_truth_label():\n",
    "    labels = {}\n",
    "    filelist = os.listdir(f\"{artifact_dir}/graph_4_6\")\n",
    "    for f in filelist:\n",
    "        labels[f] = 0\n",
    "    filelist = os.listdir(f\"{artifact_dir}/graph_4_7\")\n",
    "    for f in filelist:\n",
    "        labels[f] = 0\n",
    "\n",
    "    attack_list = [\n",
    "        '2018-04-06 11:18:26.126177915~2018-04-06 11:33:35.116170745.txt',\n",
    "        '2018-04-06 11:33:35.116170745~2018-04-06 11:48:42.606135188.txt',\n",
    "        '2018-04-06 11:48:42.606135188~2018-04-06 12:03:50.186115455.txt',\n",
    "        '2018-04-06 12:03:50.186115455~2018-04-06 14:01:32.489584227.txt',\n",
    "    ]\n",
    "    for i in attack_list:\n",
    "        labels[i] = 1\n",
    "\n",
    "    return labels\n",
    "\n",
    "def calc_attack_edges():\n",
    "    def keyword_hit(line):\n",
    "        attack_nodes = [\n",
    "            'vUgefal',\n",
    "            '/var/log/devc',\n",
    "            'nginx',\n",
    "            '81.49.200.166',\n",
    "            '78.205.235.65',\n",
    "            '200.36.109.214',\n",
    "            '139.123.0.113',\n",
    "            '152.111.159.139',\n",
    "            '61.167.39.128',\n",
    "\n",
    "        ]\n",
    "        flag = False\n",
    "        for i in attack_nodes:\n",
    "            if i in line:\n",
    "                flag = True\n",
    "                break\n",
    "        return flag\n",
    "\n",
    "    files = []\n",
    "    attack_list = [\n",
    "        '2018-04-06 11:18:26.126177915~2018-04-06 11:33:35.116170745.txt',\n",
    "        '2018-04-06 11:33:35.116170745~2018-04-06 11:48:42.606135188.txt',\n",
    "        '2018-04-06 11:48:42.606135188~2018-04-06 12:03:50.186115455.txt',\n",
    "        '2018-04-06 12:03:50.186115455~2018-04-06 14:01:32.489584227.txt',\n",
    "    ]\n",
    "    for f in attack_list:\n",
    "        files.append(f\"{artifact_dir}/graph_4_6/{f}\")\n",
    "\n",
    "    attack_edge_count = 0\n",
    "    for fpath in (files):\n",
    "        f = open(fpath)\n",
    "        for line in f:\n",
    "            if keyword_hit(line):\n",
    "                attack_edge_count += 1\n",
    "    logger.info(f\"Num of attack edges: {attack_edge_count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Start logging.\")\n",
    "\n",
    "    # Validation date\n",
    "    anomalous_queue_scores = []\n",
    "    history_list = torch.load(f\"{artifact_dir}/graph_4_5_history_list\", weights_only=False)\n",
    "    for hl in history_list:\n",
    "        anomaly_score = 0\n",
    "        for hq in hl:\n",
    "            if anomaly_score == 0:\n",
    "                # Plus 1 to ensure anomaly score is monotonically increasing\n",
    "                anomaly_score = (anomaly_score + 1) * (hq['loss'] + 1)\n",
    "            else:\n",
    "                anomaly_score = (anomaly_score) * (hq['loss'] + 1)\n",
    "        name_list = []\n",
    "\n",
    "        for i in hl:\n",
    "            name_list.append(i['name'])\n",
    "        # logger.info(f\"Constructed queue: {name_list}\")\n",
    "        # logger.info(f\"Anomaly score: {anomaly_score}\")\n",
    "\n",
    "        anomalous_queue_scores.append(anomaly_score)\n",
    "    logger.info(f\"The largest anomaly score in validation set is: {max(anomalous_queue_scores)}\\n\")\n",
    "\n",
    "\n",
    "    # Evaluating the testing set\n",
    "    pred_label = {}\n",
    "\n",
    "    filelist = os.listdir(f\"{artifact_dir}/graph_4_6/\")\n",
    "    for f in filelist:\n",
    "        pred_label[f] = 0\n",
    "\n",
    "    filelist = os.listdir(f\"{artifact_dir}/graph_4_7/\")\n",
    "    for f in filelist:\n",
    "        pred_label[f] = 0\n",
    "\n",
    "    history_list = torch.load(f\"{artifact_dir}/graph_4_6_history_list\", weights_only=False)\n",
    "    for hl in history_list:\n",
    "        anomaly_score = 0\n",
    "        for hq in hl:\n",
    "            if anomaly_score == 0:\n",
    "                anomaly_score = (anomaly_score + 1) * (hq['loss'] + 1)\n",
    "            else:\n",
    "                anomaly_score = (anomaly_score) * (hq['loss'] + 1)\n",
    "        name_list = []\n",
    "        if anomaly_score > beta_day6:\n",
    "            name_list = []\n",
    "            for i in hl:\n",
    "                name_list.append(i['name'])\n",
    "            logger.info(f\"Anomalous queue: {name_list}\")\n",
    "            for i in name_list:\n",
    "                pred_label[i] = 1\n",
    "            logger.info(f\"Anomaly score: {anomaly_score}\")\n",
    "\n",
    "    history_list = torch.load(f\"{artifact_dir}/graph_4_7_history_list\", weights_only=False)\n",
    "    for hl in history_list:\n",
    "        anomaly_score = 0\n",
    "        for hq in hl:\n",
    "            if anomaly_score == 0:\n",
    "                anomaly_score = (anomaly_score + 1) * (hq['loss'] + 1)\n",
    "            else:\n",
    "                anomaly_score = (anomaly_score) * (hq['loss'] + 1)\n",
    "        name_list = []\n",
    "        if anomaly_score > beta_day7:\n",
    "            name_list = []\n",
    "            for i in hl:\n",
    "                name_list.append(i['name'])\n",
    "            logger.info(f\"Anomalous queue: {name_list}\")\n",
    "            for i in name_list:\n",
    "                pred_label[i]=1\n",
    "            logger.info(f\"Anomaly score: {anomaly_score}\")\n",
    "\n",
    "    # Calculate the metrics\n",
    "    labels = ground_truth_label()\n",
    "    y = []\n",
    "    y_pred = []\n",
    "    for i in labels:\n",
    "        y.append(labels[i])\n",
    "        y_pred.append(pred_label[i])\n",
    "    classifier_evaluation(y, y_pred)"
   ],
   "id": "3005a0dcf1de33e1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
