{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Config",
   "id": "d67c9eec2c9316be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "#\n",
    "#                   Artifacts path\n",
    "#\n",
    "########################################################\n",
    "\n",
    "# The directory of the raw logs\n",
    "raw_dir = \"/home/anotherk/kairos312/DARPA/CADETS_E3/e3/\"\n",
    "\n",
    "# The directory to save all artifacts\n",
    "artifact_dir = \"./artifact/\"\n",
    "\n",
    "# The directory to save the vectorized graphs\n",
    "graphs_dir = artifact_dir + \"graphs/\"\n",
    "\n",
    "# The directory to save the models\n",
    "models_dir = artifact_dir + \"models/\"\n",
    "\n",
    "# The directory to save the results after testing\n",
    "test_re = artifact_dir + \"test_re/\"\n",
    "\n",
    "# The directory to save all visualized results\n",
    "vis_re = artifact_dir + \"vis_re/\"\n",
    "\n",
    "\n",
    "\n",
    "########################################################\n",
    "#\n",
    "#               Database settings\n",
    "#\n",
    "########################################################\n",
    "\n",
    "# Database name\n",
    "database = 'tc_cadet_dataset_db'\n",
    "\n",
    "# Only config this setting when you have the problem mentioned\n",
    "# in the Troubleshooting section in settings/environment-settings.md.\n",
    "# Otherwise, set it as None\n",
    "host = '/var/run/postgresql/'\n",
    "# host = None\n",
    "\n",
    "# Database user\n",
    "user = 'postgres'\n",
    "\n",
    "# The password to the database user\n",
    "password = 'postgres'\n",
    "\n",
    "# The port number for Postgres\n",
    "port = '5432'\n",
    "\n",
    "\n",
    "########################################################\n",
    "#\n",
    "#               Graph semantics\n",
    "#\n",
    "########################################################\n",
    "\n",
    "# The directions of the following edge types need to be reversed\n",
    "edge_reversed = [\n",
    "    \"EVENT_ACCEPT\",\n",
    "    \"EVENT_RECVFROM\",\n",
    "    \"EVENT_RECVMSG\"\n",
    "]\n",
    "\n",
    "# The following edges are the types only considered to construct the\n",
    "# temporal graph for experiments.\n",
    "include_edge_type=[\n",
    "    \"EVENT_WRITE\",\n",
    "    \"EVENT_READ\",\n",
    "    \"EVENT_CLOSE\",\n",
    "    \"EVENT_OPEN\",\n",
    "    \"EVENT_EXECUTE\",\n",
    "    \"EVENT_SENDTO\",\n",
    "    \"EVENT_RECVFROM\",\n",
    "]\n",
    "\n",
    "# The map between edge type and edge ID\n",
    "rel2id = {\n",
    " 1: 'EVENT_WRITE',\n",
    " 'EVENT_WRITE': 1,\n",
    " 2: 'EVENT_READ',\n",
    " 'EVENT_READ': 2,\n",
    " 3: 'EVENT_CLOSE',\n",
    " 'EVENT_CLOSE': 3,\n",
    " 4: 'EVENT_OPEN',\n",
    " 'EVENT_OPEN': 4,\n",
    " 5: 'EVENT_EXECUTE',\n",
    " 'EVENT_EXECUTE': 5,\n",
    " 6: 'EVENT_SENDTO',\n",
    " 'EVENT_SENDTO': 6,\n",
    " 7: 'EVENT_RECVFROM',\n",
    " 'EVENT_RECVFROM': 7\n",
    "}\n",
    "\n",
    "########################################################\n",
    "#\n",
    "#                   Model dimensionality\n",
    "#\n",
    "########################################################\n",
    "\n",
    "# Node Embedding Dimension\n",
    "node_embedding_dim = 16\n",
    "\n",
    "# Node State Dimension\n",
    "node_state_dim = 100\n",
    "\n",
    "# Neighborhood Sampling Size\n",
    "neighbor_size = 20\n",
    "\n",
    "# Edge Embedding Dimension\n",
    "edge_dim = 100\n",
    "\n",
    "# The time encoding Dimension\n",
    "time_dim = 100\n",
    "\n",
    "\n",
    "########################################################\n",
    "#\n",
    "#                   Train&Test\n",
    "#\n",
    "########################################################\n",
    "\n",
    "# Batch size for training and testing\n",
    "BATCH = 1024\n",
    "\n",
    "# Parameters for optimizer\n",
    "lr=0.00005\n",
    "eps=1e-08\n",
    "weight_decay=0.01\n",
    "\n",
    "epoch_num=50\n",
    "\n",
    "# The size of time window, 60000000000 represent 1 min in nanoseconds.\n",
    "# The default setting is 15 minutes.\n",
    "time_window_size = 60000000000 * 15\n",
    "\n",
    "\n",
    "########################################################\n",
    "#\n",
    "#                   Threshold\n",
    "#\n",
    "########################################################\n",
    "\n",
    "beta_day6 = 100\n",
    "beta_day7 = 100\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Utilities",
   "id": "fe2c596a2ac5b7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pytz\n",
    "from time import mktime\n",
    "from datetime import datetime\n",
    "import time\n",
    "import psycopg2\n",
    "from psycopg2 import extras as ex\n",
    "import os.path as osp\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from torch_geometric.data import TemporalData\n",
    "from torch_geometric.nn import TGNMemory, TransformerConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.models.tgn import (LastNeighborLoader, IdentityMessage, MeanAggregator,\n",
    "                                           LastAggregator)\n",
    "from torch_geometric import *\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import xxhash\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "def ns_time_to_datetime(ns):\n",
    "    \"\"\"\n",
    "    :param ns: int nano timestamp\n",
    "    :return: datetime   format: 2013-10-10 23:40:00.000000000\n",
    "    \"\"\"\n",
    "    dt = datetime.fromtimestamp(int(ns) // 1000000000)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    s += '.' + str(int(int(ns) % 1000000000)).zfill(9)\n",
    "    return s\n",
    "\n",
    "def ns_time_to_datetime_US(ns):\n",
    "    \"\"\"\n",
    "    :param ns: int nano timestamp\n",
    "    :return: datetime   format: 2013-10-10 23:40:00.000000000\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    dt = pytz.datetime.datetime.fromtimestamp(int(ns) // 1000000000, tz)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    s += '.' + str(int(int(ns) % 1000000000)).zfill(9)\n",
    "    return s\n",
    "\n",
    "def time_to_datetime_US(s):\n",
    "    \"\"\"\n",
    "    :param ns: int nano timestamp\n",
    "    :return: datetime   format: 2013-10-10 23:40:00\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    dt = pytz.datetime.datetime.fromtimestamp(int(s), tz)\n",
    "    s = dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    return s\n",
    "\n",
    "def datetime_to_ns_time(date):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    timeStamp = int(time.mktime(timeArray))\n",
    "    timeStamp = timeStamp * 1000000000\n",
    "    return timeStamp\n",
    "\n",
    "def datetime_to_ns_time_US(date):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    dt = datetime.fromtimestamp(mktime(timeArray))\n",
    "    timestamp = tz.localize(dt)\n",
    "    timestamp = timestamp.timestamp()\n",
    "    timeStamp = timestamp * 1000000000\n",
    "    return int(timeStamp)\n",
    "\n",
    "def datetime_to_timestamp_US(date):\n",
    "    \"\"\"\n",
    "    :param date: str   format: %Y-%m-%d %H:%M:%S   e.g. 2013-10-10 23:40:00\n",
    "    :return: nano timestamp\n",
    "    \"\"\"\n",
    "    tz = pytz.timezone('US/Eastern')\n",
    "    timeArray = time.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    dt = datetime.fromtimestamp(mktime(timeArray))\n",
    "    timestamp = tz.localize(dt)\n",
    "    timestamp = timestamp.timestamp()\n",
    "    timeStamp = timestamp\n",
    "    return int(timeStamp)\n",
    "\n",
    "def init_database_connection():\n",
    "    if host is not None:\n",
    "        if host == \"/var/run/postgresql/\":\n",
    "            connect = psycopg2.connect(database = database,\n",
    "                                       host = host,\n",
    "                                       user = user,\n",
    "                                       password = password,\n",
    "                                       port = port\n",
    "                                      )\n",
    "        else:\n",
    "            connect = psycopg2.connect(database=database,\n",
    "                                       host=host,\n",
    "                                       user=user,\n",
    "                                       password=password,\n",
    "                                       port=port,\n",
    "                                       sslmode=\"require\"\n",
    "                                       )\n",
    "    else:\n",
    "        connect = psycopg2.connect(database = database,\n",
    "                                   user = user,\n",
    "                                   password = password,\n",
    "                                   port = port\n",
    "                                  )\n",
    "    cur = connect.cursor()\n",
    "    return cur, connect\n",
    "\n",
    "def gen_nodeid2msg(cur):\n",
    "    sql = \"select * from node2id ORDER BY index_id;\"\n",
    "    cur.execute(sql)\n",
    "    rows = cur.fetchall()\n",
    "    nodeid2msg = {}\n",
    "    for i in rows:\n",
    "        nodeid2msg[i[0]] = i[-1]\n",
    "        nodeid2msg[i[-1]] = {i[1]: i[2]}\n",
    "\n",
    "    return nodeid2msg\n",
    "\n",
    "def tensor_find(t,x):\n",
    "    t_np=t.cpu().numpy()\n",
    "    idx=np.argwhere(t_np==x)\n",
    "    return idx[0][0]+1\n",
    "\n",
    "def std(t):\n",
    "    t = np.array(t)\n",
    "    return np.std(t)\n",
    "\n",
    "def var(t):\n",
    "    t = np.array(t)\n",
    "    return np.var(t)\n",
    "\n",
    "def mean(t):\n",
    "    t = np.array(t)\n",
    "    return np.mean(t)\n",
    "\n",
    "def hashgen(l):\n",
    "    \"\"\"Generate a single hash value from a list. @l is a list of\n",
    "    string values, which can be properties of a node/edge. This\n",
    "    function returns a single hashed integer value.\"\"\"\n",
    "    hasher = xxhash.xxh64()\n",
    "    for e in l:\n",
    "        hasher.update(e)\n",
    "    return hasher.intdigest()"
   ],
   "id": "23176483338e52ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocess",
   "id": "e9aaef5a19906a35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!mkdir -p ./artifact/",
   "id": "7327760a8068a3c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Database",
   "id": "4613dce29a272357"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "\n",
    "filelist = ['ta1-cadets-e3-official.json',\n",
    " 'ta1-cadets-e3-official.json.1',\n",
    " 'ta1-cadets-e3-official.json.2',\n",
    " 'ta1-cadets-e3-official-1.json',\n",
    " 'ta1-cadets-e3-official-1.json.1',\n",
    " 'ta1-cadets-e3-official-1.json.2',\n",
    " 'ta1-cadets-e3-official-1.json.3',\n",
    " 'ta1-cadets-e3-official-1.json.4',\n",
    " 'ta1-cadets-e3-official-2.json',\n",
    " 'ta1-cadets-e3-official-2.json.1']\n",
    "\n",
    "\n",
    "def stringtomd5(originstr):\n",
    "    originstr = originstr.encode(\"utf-8\")\n",
    "    signaturemd5 = hashlib.sha256()\n",
    "    signaturemd5.update(originstr)\n",
    "    return signaturemd5.hexdigest()\n",
    "\n",
    "def store_netflow(file_path, cur, connect):\n",
    "    # Parse data from logs\n",
    "    netobjset = set()\n",
    "    netobj2hash = {}\n",
    "    for file in tqdm(filelist):\n",
    "        with open(file_path + file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if \"NetFlowObject\" in line:\n",
    "                    try:\n",
    "                        res = re.findall(\n",
    "                            'NetFlowObject\":{\"uuid\":\"(.*?)\"(.*?)\"localAddress\":\"(.*?)\",\"localPort\":(.*?),\"remoteAddress\":\"(.*?)\",\"remotePort\":(.*?),',\n",
    "                            line)[0]\n",
    "\n",
    "                        nodeid = res[0]\n",
    "                        srcaddr = res[2]\n",
    "                        srcport = res[3]\n",
    "                        dstaddr = res[4]\n",
    "                        dstport = res[5]\n",
    "\n",
    "                        nodeproperty = srcaddr + \",\" + srcport + \",\" + dstaddr + \",\" + dstport\n",
    "                        hashstr = stringtomd5(nodeproperty)\n",
    "                        netobj2hash[nodeid] = [hashstr, nodeproperty]\n",
    "                        netobj2hash[hashstr] = nodeid\n",
    "                        netobjset.add(hashstr)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "    # Store data into database\n",
    "    datalist = []\n",
    "    for i in netobj2hash.keys():\n",
    "        if len(i) != 64:\n",
    "            datalist.append([i] + [netobj2hash[i][0]] + netobj2hash[i][1].split(\",\"))\n",
    "\n",
    "    sql = '''insert into netflow_node_table\n",
    "                         values %s\n",
    "            '''\n",
    "    ex.execute_values(cur, sql, datalist, page_size=10000)\n",
    "    connect.commit()\n",
    "\n",
    "def store_subject(file_path, cur, connect):\n",
    "    # Parse data from logs\n",
    "    scusess_count = 0\n",
    "    fail_count = 0\n",
    "    subject_objset = set()\n",
    "    subject_obj2hash = {}  #\n",
    "    for file in tqdm(filelist):\n",
    "        with open(file_path + file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if \"Event\" in line:\n",
    "                    subject_uuid = re.findall(\n",
    "                        '\"subject\":{\"com.bbn.tc.schema.avro.cdm18.UUID\":\"(.*?)\"}(.*?)\"exec\":\"(.*?)\"', line)\n",
    "                    try:\n",
    "                        subject_obj2hash[subject_uuid[0][0]] = subject_uuid[0][-1]\n",
    "                        scusess_count += 1\n",
    "                    except:\n",
    "                        try:\n",
    "                            subject_obj2hash[subject_uuid[0][0]] = \"null\"\n",
    "                        except:\n",
    "                            pass\n",
    "                        fail_count += 1\n",
    "    # Store into database\n",
    "    datalist = []\n",
    "    for i in subject_obj2hash.keys():\n",
    "        if len(i) != 64:\n",
    "            datalist.append([i] + [stringtomd5(subject_obj2hash[i]), subject_obj2hash[i]])\n",
    "    sql = '''insert into subject_node_table\n",
    "                         values %s\n",
    "            '''\n",
    "    ex.execute_values(cur, sql, datalist, page_size=10000)\n",
    "    connect.commit()\n",
    "\n",
    "def store_file(file_path, cur, connect):\n",
    "    file_node = set()\n",
    "    for file in tqdm(filelist):\n",
    "        with open(file_path + file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if \"com.bbn.tc.schema.avro.cdm18.FileObject\" in line:\n",
    "                    Object_uuid = re.findall('FileObject\":{\"uuid\":\"(.*?)\",', line)\n",
    "                    try:\n",
    "                        file_node.add(Object_uuid[0])\n",
    "                    except:\n",
    "                        print(line)\n",
    "\n",
    "    file_obj2hash = {}\n",
    "    for file in tqdm(filelist):\n",
    "        with open(file_path + file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if '{\"datum\":{\"com.bbn.tc.schema.avro.cdm18.Event\"' in line:\n",
    "                    predicateObject_uuid = re.findall('\"predicateObject\":{\"com.bbn.tc.schema.avro.cdm18.UUID\":\"(.*?)\"}',\n",
    "                                                      line)\n",
    "                    if len(predicateObject_uuid) > 0:\n",
    "                        if predicateObject_uuid[0] in file_node:\n",
    "                            if '\"predicateObjectPath\":null,' not in line and '<unknown>' not in line:\n",
    "                                path_name = re.findall('\"predicateObjectPath\":{\"string\":\"(.*?)\"', line)\n",
    "                                file_obj2hash[predicateObject_uuid[0]] = path_name\n",
    "\n",
    "    datalist = []\n",
    "    for i in file_obj2hash.keys():\n",
    "        if len(i) != 64:\n",
    "            datalist.append([i] + [stringtomd5(file_obj2hash[i][0]), file_obj2hash[i][0]])\n",
    "    sql = '''insert into file_node_table\n",
    "                         values %s\n",
    "            '''\n",
    "    ex.execute_values(cur, sql, datalist, page_size=10000)\n",
    "    connect.commit()\n",
    "\n",
    "def create_node_list(cur, connect):\n",
    "    node_list = {}\n",
    "\n",
    "    # file\n",
    "    sql = \"\"\"\n",
    "    select * from file_node_table;\n",
    "    \"\"\"\n",
    "    cur.execute(sql)\n",
    "    records = cur.fetchall()\n",
    "\n",
    "    for i in records:\n",
    "        node_list[i[1]] = [\"file\", i[-1]]\n",
    "    file_uuid2hash = {}\n",
    "    for i in records:\n",
    "        file_uuid2hash[i[0]] = i[1]\n",
    "\n",
    "    # subject\n",
    "    sql = \"\"\"\n",
    "    select * from subject_node_table;\n",
    "    \"\"\"\n",
    "    cur.execute(sql)\n",
    "    records = cur.fetchall()\n",
    "    for i in records:\n",
    "        node_list[i[1]] = [\"subject\", i[-1]]\n",
    "    subject_uuid2hash = {}\n",
    "    for i in records:\n",
    "        subject_uuid2hash[i[0]] = i[1]\n",
    "\n",
    "    # netflow\n",
    "    sql = \"\"\"\n",
    "    select * from netflow_node_table;\n",
    "    \"\"\"\n",
    "    cur.execute(sql)\n",
    "    records = cur.fetchall()\n",
    "    for i in records:\n",
    "        node_list[i[1]] = [\"netflow\", i[-2] + \":\" + i[-1]]\n",
    "\n",
    "    net_uuid2hash = {}\n",
    "    for i in records:\n",
    "        net_uuid2hash[i[0]] = i[1]\n",
    "\n",
    "    node_list_database = []\n",
    "    node_index = 0\n",
    "    for i in node_list:\n",
    "        node_list_database.append([i] + node_list[i] + [node_index])\n",
    "        node_index += 1\n",
    "\n",
    "    sql = '''insert into node2id\n",
    "                         values %s\n",
    "            '''\n",
    "    ex.execute_values(cur, sql, node_list_database, page_size=10000)\n",
    "    connect.commit()\n",
    "\n",
    "    sql = \"select * from node2id ORDER BY index_id;\"\n",
    "    cur.execute(sql)\n",
    "    rows = cur.fetchall()\n",
    "    nodeid2msg = {}\n",
    "    for i in rows:\n",
    "        nodeid2msg[i[0]] = i[-1]\n",
    "        nodeid2msg[i[-1]] = {i[1]: i[2]}\n",
    "\n",
    "    return nodeid2msg, subject_uuid2hash, file_uuid2hash, net_uuid2hash\n",
    "\n",
    "def store_event(file_path, cur, connect, reverse, nodeid2msg, subject_uuid2hash, file_uuid2hash, net_uuid2hash):\n",
    "    datalist = []\n",
    "    for file in tqdm(filelist):\n",
    "        with open(file_path + file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if '{\"datum\":{\"com.bbn.tc.schema.avro.cdm18.Event\"' in line and \"EVENT_FLOWS_TO\" not in line:\n",
    "                    subject_uuid = re.findall('\"subject\":{\"com.bbn.tc.schema.avro.cdm18.UUID\":\"(.*?)\"}', line)\n",
    "                    predicateObject_uuid = re.findall('\"predicateObject\":{\"com.bbn.tc.schema.avro.cdm18.UUID\":\"(.*?)\"}', line)\n",
    "                    if len(subject_uuid) > 0 and len(predicateObject_uuid) > 0:\n",
    "                        if subject_uuid[0] in subject_uuid2hash and (predicateObject_uuid[0] in file_uuid2hash or predicateObject_uuid[0] in net_uuid2hash):\n",
    "                            relation_type = re.findall('\"type\":\"(.*?)\"', line)[0]\n",
    "                            time_rec = re.findall('\"timestampNanos\":(.*?),', line)[0]\n",
    "                            time_rec = int(time_rec)\n",
    "                            subjectId = subject_uuid2hash[subject_uuid[0]]\n",
    "                            if predicateObject_uuid[0] in file_uuid2hash:\n",
    "                                objectId = file_uuid2hash[predicateObject_uuid[0]]\n",
    "                            else:\n",
    "                                objectId = net_uuid2hash[predicateObject_uuid[0]]\n",
    "                            if relation_type in reverse:\n",
    "                                datalist.append(\n",
    "                                    [objectId, nodeid2msg[objectId], relation_type, subjectId, nodeid2msg[subjectId],\n",
    "                                     time_rec])\n",
    "                            else:\n",
    "                                datalist.append(\n",
    "                                    [subjectId, nodeid2msg[subjectId], relation_type, objectId, nodeid2msg[objectId],\n",
    "                                     time_rec])\n",
    "\n",
    "    sql = '''insert into event_table\n",
    "                         values %s\n",
    "            '''\n",
    "    ex.execute_values(cur, sql, datalist, page_size=10000)\n",
    "    connect.commit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cur, connect = init_database_connection()\n",
    "\n",
    "    # There will be 155322 netflow nodes stored in the table\n",
    "    print(\"Processing netflow data\")\n",
    "    store_netflow(file_path=raw_dir, cur=cur, connect=connect)\n",
    "\n",
    "    # There will be 224146 subject nodes stored in the table\n",
    "    print(\"Processing subject data\")\n",
    "    store_subject(file_path=raw_dir, cur=cur, connect=connect)\n",
    "\n",
    "    # There will be 234245 file nodes stored in the table\n",
    "    print(\"Processing file data\")\n",
    "    store_file(file_path=raw_dir, cur=cur, connect=connect)\n",
    "\n",
    "    # There will be 268242 entities stored in the table\n",
    "    print(\"Extracting the node list\")\n",
    "    nodeid2msg, subject_uuid2hash, file_uuid2hash, net_uuid2hash = create_node_list(cur=cur, connect=connect)\n",
    "\n",
    "    # There will be 29727441 events stored in the table\n",
    "    print(\"Processing the events\")\n",
    "    store_event(\n",
    "        file_path=raw_dir,\n",
    "        cur=cur,\n",
    "        connect=connect,\n",
    "        reverse=edge_reversed,\n",
    "        nodeid2msg=nodeid2msg,\n",
    "        subject_uuid2hash=subject_uuid2hash,\n",
    "        file_uuid2hash=file_uuid2hash,\n",
    "        net_uuid2hash=net_uuid2hash\n",
    "    )"
   ],
   "id": "f6b94f404e76717f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
